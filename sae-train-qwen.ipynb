{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5O8tQblzOVHu"
   },
   "source": [
    "# Training a basic SAE with SAELens\n",
    "\n",
    "This notebook is derived from the SAELens [training tutorial](https://github.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb). I ran it on a g6.12xlarge, and training took about 11 hours.\n",
    "\n",
    "In order to use SAELens, you need to pick a model that's supported by [TransformerLens](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html). \n",
    "I tried to pick a model that didn't yet have a [published SAE](https://jbloomaus.github.io/SAELens/sae_table/). I selected `Qwen/Qwen2.5-1.5B-Instruct`.\n",
    "\n",
    "As part of the training process, you can pick any pretraining dataset from [HuggingFace](https://huggingface.co/datasets). You do not need to use a dataset with any of your own information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LeRi_tw2dhae",
    "outputId": "b2085c63-5296-4470-8247-6ee89e460607"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython  # type: ignore\n",
    "\n",
    "ipython = get_ipython()\n",
    "assert ipython is not None\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uy-b3CcSOVHu",
    "outputId": "58ce28d0-f91f-436d-cf87-76bb26e2ecaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block is directly from the SAE tutorial, except for specifying the model name and disabling logging to [WandB](https://wandb.ai/site)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oAsZCAdJOVHw",
    "outputId": "48ba80d1-d062-45fb-d817-f4b81d0e58a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-5-LR-5e-05-Tokens-1.229e+08\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 30000\n",
      "Total wandb updates: 1000\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 30 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    }
   ],
   "source": [
    "total_training_steps = 30000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_layer=20,  # Only one layer in the model.\n",
    "    d_in=1536,  # the width of the mlp output.\n",
    "    dataset_path=\"Skylion007/openwebtext\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=False,\n",
    "    streaming=True,  # we could pre-download the token dataset if it was small.\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=\"expected_average_only_in\",\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=False,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    act_store_device='cpu',\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/sae_lens/training/activations_store.py:246: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n",
      "Training SAE:   0%|                                                                                                            | 0/122880000 [00:00<?, ?it/s]\n",
      "Estimating norm scaling factor:   0%|                                                                                               | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "Estimating norm scaling factor:   0%|                                                                                   | 1/1000 [02:10<36:04:41, 130.01s/it]\u001b[A\n",
      "Estimating norm scaling factor:   1%|▊                                                                                   | 10/1000 [02:10<2:34:59,  9.39s/it]\u001b[A\n",
      "Estimating norm scaling factor:   2%|█▋                                                                                  | 20/1000 [02:10<1:02:09,  3.81s/it]\u001b[A\n",
      "Estimating norm scaling factor:   3%|██▌                                                                                   | 30/1000 [02:10<33:19,  2.06s/it]\u001b[A\n",
      "Estimating norm scaling factor:   4%|███▍                                                                                  | 40/1000 [02:10<19:58,  1.25s/it]\u001b[A\n",
      "Estimating norm scaling factor:   5%|████▎                                                                                 | 50/1000 [02:10<12:40,  1.25it/s]\u001b[A\n",
      "Estimating norm scaling factor:   6%|█████▏                                                                                | 60/1000 [02:10<08:20,  1.88it/s]\u001b[A\n",
      "Estimating norm scaling factor:   6%|█████▏                                                                                | 60/1000 [02:29<08:20,  1.88it/s]\u001b[A\n",
      "Estimating norm scaling factor:   6%|█████▌                                                                                | 65/1000 [03:16<46:33,  2.99s/it]\u001b[A\n",
      "Estimating norm scaling factor:   8%|██████▍                                                                               | 75/1000 [03:16<29:36,  1.92s/it]\u001b[A\n",
      "Estimating norm scaling factor:   8%|███████▏                                                                              | 84/1000 [03:16<20:07,  1.32s/it]\u001b[A\n",
      "Estimating norm scaling factor:   9%|███████▉                                                                              | 93/1000 [03:16<13:47,  1.10it/s]\u001b[A\n",
      "Estimating norm scaling factor:  10%|████████▋                                                                            | 102/1000 [03:16<09:30,  1.58it/s]\u001b[A\n",
      "Estimating norm scaling factor:  11%|█████████▍                                                                           | 111/1000 [03:16<06:35,  2.25it/s]\u001b[A\n",
      "Estimating norm scaling factor:  12%|██████████▏                                                                          | 120/1000 [03:16<04:35,  3.20it/s]\u001b[A\n",
      "Estimating norm scaling factor:  12%|██████████▏                                                                          | 120/1000 [03:29<04:35,  3.20it/s]\u001b[A\n",
      "Estimating norm scaling factor:  13%|██████████▉                                                                          | 129/1000 [04:23<35:34,  2.45s/it]\u001b[A\n",
      "Estimating norm scaling factor:  14%|███████████▋                                                                         | 138/1000 [04:23<24:37,  1.71s/it]\u001b[A\n",
      "Estimating norm scaling factor:  15%|████████████▍                                                                        | 147/1000 [04:23<17:09,  1.21s/it]\u001b[A\n",
      "Estimating norm scaling factor:  16%|█████████████▎                                                                       | 156/1000 [04:23<11:54,  1.18it/s]\u001b[A\n",
      "Estimating norm scaling factor:  16%|██████████████                                                                       | 165/1000 [04:23<08:17,  1.68it/s]\u001b[A\n",
      "Estimating norm scaling factor:  17%|██████████████▊                                                                      | 174/1000 [04:23<05:47,  2.38it/s]\u001b[A\n",
      "Estimating norm scaling factor:  18%|███████████████▌                                                                     | 183/1000 [04:23<04:02,  3.36it/s]\u001b[A\n",
      "Estimating norm scaling factor:  19%|████████████████▎                                                                    | 192/1000 [04:23<02:50,  4.73it/s]\u001b[A\n",
      "Estimating norm scaling factor:  19%|████████████████▎                                                                    | 192/1000 [04:39<02:50,  4.73it/s]\u001b[A\n",
      "Estimating norm scaling factor:  19%|████████████████▍                                                                    | 193/1000 [05:30<43:29,  3.23s/it]\u001b[A\n",
      "Estimating norm scaling factor:  20%|█████████████████▏                                                                   | 202/1000 [05:30<27:11,  2.04s/it]\u001b[A\n",
      "Estimating norm scaling factor:  21%|██████████████████                                                                   | 212/1000 [05:30<16:59,  1.29s/it]\u001b[A\n",
      "Estimating norm scaling factor:  22%|██████████████████▊                                                                  | 222/1000 [05:30<11:01,  1.18it/s]\u001b[A\n",
      "Estimating norm scaling factor:  23%|███████████████████▋                                                                 | 232/1000 [05:31<07:20,  1.75it/s]\u001b[A\n",
      "Estimating norm scaling factor:  24%|████████████████████▌                                                                | 242/1000 [05:31<04:57,  2.55it/s]\u001b[A\n",
      "Estimating norm scaling factor:  25%|█████████████████████▍                                                               | 252/1000 [05:31<03:24,  3.66it/s]\u001b[A\n",
      "Estimating norm scaling factor:  25%|█████████████████████▍                                                               | 252/1000 [05:49<03:24,  3.66it/s]\u001b[A\n",
      "Estimating norm scaling factor:  26%|█████████████████████▊                                                               | 257/1000 [06:37<32:57,  2.66s/it]\u001b[A\n",
      "Estimating norm scaling factor:  27%|██████████████████████▌                                                              | 266/1000 [06:37<22:10,  1.81s/it]\u001b[A\n",
      "Estimating norm scaling factor:  28%|███████████████████████▍                                                             | 276/1000 [06:38<14:32,  1.21s/it]\u001b[A\n",
      "Estimating norm scaling factor:  29%|████████████████████████▎                                                            | 286/1000 [06:38<09:43,  1.22it/s]\u001b[A\n",
      "Estimating norm scaling factor:  30%|█████████████████████████▏                                                           | 296/1000 [06:38<06:39,  1.76it/s]\u001b[A\n",
      "Estimating norm scaling factor:  31%|██████████████████████████                                                           | 306/1000 [06:38<04:33,  2.54it/s]\u001b[A\n",
      "Estimating norm scaling factor:  32%|██████████████████████████▊                                                          | 316/1000 [06:38<03:08,  3.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  32%|██████████████████████████▊                                                          | 316/1000 [06:49<03:08,  3.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  32%|███████████████████████████▎                                                         | 321/1000 [07:44<29:18,  2.59s/it]\u001b[A\n",
      "Estimating norm scaling factor:  33%|████████████████████████████                                                         | 330/1000 [07:44<19:47,  1.77s/it]\u001b[A\n",
      "Estimating norm scaling factor:  34%|████████████████████████████▉                                                        | 340/1000 [07:44<13:00,  1.18s/it]\u001b[A\n",
      "Estimating norm scaling factor:  35%|█████████████████████████████▋                                                       | 350/1000 [07:44<08:42,  1.24it/s]\u001b[A\n",
      "Estimating norm scaling factor:  36%|██████████████████████████████▌                                                      | 360/1000 [07:44<05:54,  1.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor:  37%|███████████████████████████████▎                                                     | 369/1000 [07:44<04:10,  2.52it/s]\u001b[A\n",
      "Estimating norm scaling factor:  38%|████████████████████████████████▏                                                    | 379/1000 [07:44<02:51,  3.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  38%|████████████████████████████████▏                                                    | 379/1000 [07:59<02:51,  3.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  38%|████████████████████████████████▋                                                    | 385/1000 [08:49<25:44,  2.51s/it]\u001b[A\n",
      "Estimating norm scaling factor:  39%|█████████████████████████████████▍                                                   | 394/1000 [08:49<17:27,  1.73s/it]\u001b[A\n",
      "Estimating norm scaling factor:  40%|██████████████████████████████████▎                                                  | 404/1000 [08:49<11:30,  1.16s/it]\u001b[A\n",
      "Estimating norm scaling factor:  41%|███████████████████████████████████▏                                                 | 414/1000 [08:49<07:43,  1.27it/s]\u001b[A\n",
      "Estimating norm scaling factor:  42%|███████████████████████████████████▉                                                 | 423/1000 [08:50<05:24,  1.78it/s]\u001b[A\n",
      "Estimating norm scaling factor:  43%|████████████████████████████████████▊                                                | 433/1000 [08:50<03:39,  2.58it/s]\u001b[A\n",
      "Estimating norm scaling factor:  44%|█████████████████████████████████████▌                                               | 442/1000 [08:50<02:38,  3.52it/s]\u001b[A\n",
      "Estimating norm scaling factor:  44%|█████████████████████████████████████▌                                               | 442/1000 [09:09<02:38,  3.52it/s]\u001b[A\n",
      "Estimating norm scaling factor:  45%|██████████████████████████████████████▏                                              | 449/1000 [09:55<22:39,  2.47s/it]\u001b[A\n",
      "Estimating norm scaling factor:  46%|███████████████████████████████████████                                              | 459/1000 [09:55<14:53,  1.65s/it]\u001b[A\n",
      "Estimating norm scaling factor:  47%|███████████████████████████████████████▊                                             | 469/1000 [09:55<09:57,  1.12s/it]\u001b[A\n",
      "Estimating norm scaling factor:  48%|████████████████████████████████████████▋                                            | 479/1000 [09:55<06:43,  1.29it/s]\u001b[A\n",
      "Estimating norm scaling factor:  49%|█████████████████████████████████████████▌                                           | 489/1000 [09:55<04:34,  1.86it/s]\u001b[A\n",
      "Estimating norm scaling factor:  50%|██████████████████████████████████████████▍                                          | 499/1000 [09:55<03:08,  2.66it/s]\u001b[A\n",
      "Estimating norm scaling factor:  51%|███████████████████████████████████████████▎                                         | 509/1000 [09:55<02:09,  3.78it/s]\u001b[A\n",
      "Estimating norm scaling factor:  51%|███████████████████████████████████████████▎                                         | 509/1000 [10:09<02:09,  3.78it/s]\u001b[A\n",
      "Estimating norm scaling factor:  51%|███████████████████████████████████████████▌                                         | 513/1000 [11:01<21:29,  2.65s/it]\u001b[A\n",
      "Estimating norm scaling factor:  52%|████████████████████████████████████████████▎                                        | 522/1000 [11:01<14:18,  1.80s/it]\u001b[A\n",
      "Estimating norm scaling factor:  53%|█████████████████████████████████████████████▏                                       | 532/1000 [11:01<09:17,  1.19s/it]\u001b[A\n",
      "Estimating norm scaling factor:  54%|██████████████████████████████████████████████                                       | 542/1000 [11:01<06:09,  1.24it/s]\u001b[A\n",
      "Estimating norm scaling factor:  55%|██████████████████████████████████████████████▉                                      | 552/1000 [11:01<04:07,  1.81it/s]\u001b[A\n",
      "Estimating norm scaling factor:  56%|███████████████████████████████████████████████▊                                     | 562/1000 [11:01<02:48,  2.60it/s]\u001b[A\n",
      "Estimating norm scaling factor:  57%|████████████████████████████████████████████████▌                                    | 572/1000 [11:01<01:55,  3.72it/s]\u001b[A\n",
      "Estimating norm scaling factor:  57%|████████████████████████████████████████████████▌                                    | 572/1000 [11:19<01:55,  3.72it/s]\u001b[A\n",
      "Estimating norm scaling factor:  58%|█████████████████████████████████████████████████                                    | 577/1000 [12:07<18:22,  2.61s/it]\u001b[A\n",
      "Estimating norm scaling factor:  58%|█████████████████████████████████████████████████▌                                   | 583/1000 [12:08<13:53,  2.00s/it]\u001b[A\n",
      "Estimating norm scaling factor:  59%|██████████████████████████████████████████████████▍                                  | 593/1000 [12:08<08:42,  1.28s/it]\u001b[A\n",
      "Estimating norm scaling factor:  60%|███████████████████████████████████████████████████▎                                 | 603/1000 [12:08<05:37,  1.17it/s]\u001b[A\n",
      "Estimating norm scaling factor:  61%|████████████████████████████████████████████████████                                 | 612/1000 [12:08<03:51,  1.68it/s]\u001b[A\n",
      "Estimating norm scaling factor:  62%|████████████████████████████████████████████████████▊                                | 622/1000 [12:08<02:33,  2.46it/s]\u001b[A\n",
      "Estimating norm scaling factor:  63%|█████████████████████████████████████████████████████▋                               | 631/1000 [12:08<01:46,  3.46it/s]\u001b[A\n",
      "Estimating norm scaling factor:  63%|█████████████████████████████████████████████████████▋                               | 631/1000 [12:19<01:46,  3.46it/s]\u001b[A\n",
      "Estimating norm scaling factor:  64%|██████████████████████████████████████████████████████▍                              | 641/1000 [13:14<13:47,  2.31s/it]\u001b[A\n",
      "Estimating norm scaling factor:  65%|███████████████████████████████████████████████████████▎                             | 650/1000 [13:14<09:33,  1.64s/it]\u001b[A\n",
      "Estimating norm scaling factor:  66%|████████████████████████████████████████████████████████                             | 660/1000 [13:14<06:22,  1.12s/it]\u001b[A\n",
      "Estimating norm scaling factor:  67%|████████████████████████████████████████████████████████▉                            | 670/1000 [13:15<04:16,  1.29it/s]\u001b[A\n",
      "Estimating norm scaling factor:  68%|█████████████████████████████████████████████████████████▊                           | 680/1000 [13:15<02:53,  1.84it/s]\u001b[A\n",
      "Estimating norm scaling factor:  69%|██████████████████████████████████████████████████████████▋                          | 690/1000 [13:15<01:57,  2.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  70%|███████████████████████████████████████████████████████████▍                         | 700/1000 [13:15<01:20,  3.73it/s]\u001b[A\n",
      "Estimating norm scaling factor:  70%|███████████████████████████████████████████████████████████▍                         | 700/1000 [13:29<01:20,  3.73it/s]\u001b[A\n",
      "Estimating norm scaling factor:  70%|███████████████████████████████████████████████████████████▉                         | 705/1000 [14:21<12:37,  2.57s/it]\u001b[A\n",
      "Estimating norm scaling factor:  71%|████████████████████████████████████████████████████████████▌                        | 713/1000 [14:21<08:44,  1.83s/it]\u001b[A\n",
      "Estimating norm scaling factor:  72%|█████████████████████████████████████████████████████████████▍                       | 723/1000 [14:21<05:34,  1.21s/it]\u001b[A\n",
      "Estimating norm scaling factor:  73%|██████████████████████████████████████████████████████████████▎                      | 733/1000 [14:21<03:38,  1.22it/s]\u001b[A\n",
      "Estimating norm scaling factor:  74%|███████████████████████████████████████████████████████████████                      | 742/1000 [14:21<02:30,  1.71it/s]\u001b[A\n",
      "Estimating norm scaling factor:  75%|███████████████████████████████████████████████████████████████▉                     | 752/1000 [14:22<01:39,  2.50it/s]\u001b[A\n",
      "Estimating norm scaling factor:  76%|████████████████████████████████████████████████████████████████▊                    | 762/1000 [14:22<01:06,  3.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor:  76%|████████████████████████████████████████████████████████████████▊                    | 762/1000 [14:39<01:06,  3.59it/s]\u001b[A\n",
      "Estimating norm scaling factor:  77%|█████████████████████████████████████████████████████████████████▎                   | 769/1000 [15:28<09:33,  2.48s/it]\u001b[A\n",
      "Estimating norm scaling factor:  78%|██████████████████████████████████████████████████████████████████▏                  | 778/1000 [15:28<06:23,  1.73s/it]\u001b[A\n",
      "Estimating norm scaling factor:  79%|██████████████████████████████████████████████████████████████████▉                  | 788/1000 [15:28<04:07,  1.17s/it]\u001b[A\n",
      "Estimating norm scaling factor:  80%|███████████████████████████████████████████████████████████████████▊                 | 798/1000 [15:29<02:41,  1.25it/s]\u001b[A\n",
      "Estimating norm scaling factor:  81%|████████████████████████████████████████████████████████████████████▋                | 808/1000 [15:29<01:46,  1.80it/s]\u001b[A\n",
      "Estimating norm scaling factor:  82%|█████████████████████████████████████████████████████████████████████▌               | 818/1000 [15:29<01:10,  2.59it/s]\u001b[A\n",
      "Estimating norm scaling factor:  83%|██████████████████████████████████████████████████████████████████████▍              | 828/1000 [15:29<00:46,  3.68it/s]\u001b[A\n",
      "Estimating norm scaling factor:  83%|██████████████████████████████████████████████████████████████████████▍              | 828/1000 [15:39<00:46,  3.68it/s]\u001b[A\n",
      "Estimating norm scaling factor:  83%|██████████████████████████████████████████████████████████████████████▊              | 833/1000 [16:35<07:14,  2.60s/it]\u001b[A\n",
      "Estimating norm scaling factor:  84%|███████████████████████████████████████████████████████████████████████▌             | 842/1000 [16:35<04:41,  1.78s/it]\u001b[A\n",
      "Estimating norm scaling factor:  85%|████████████████████████████████████████████████████████████████████████▍            | 852/1000 [16:35<02:56,  1.19s/it]\u001b[A\n",
      "Estimating norm scaling factor:  86%|█████████████████████████████████████████████████████████████████████████▎           | 862/1000 [16:35<01:51,  1.23it/s]\u001b[A\n",
      "Estimating norm scaling factor:  87%|██████████████████████████████████████████████████████████████████████████           | 872/1000 [16:36<01:11,  1.79it/s]\u001b[A\n",
      "Estimating norm scaling factor:  88%|██████████████████████████████████████████████████████████████████████████▉          | 882/1000 [16:36<00:46,  2.54it/s]\u001b[A\n",
      "Estimating norm scaling factor:  89%|███████████████████████████████████████████████████████████████████████████▊         | 892/1000 [16:36<00:29,  3.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  89%|███████████████████████████████████████████████████████████████████████████▊         | 892/1000 [16:49<00:29,  3.63it/s]\u001b[A\n",
      "Estimating norm scaling factor:  90%|████████████████████████████████████████████████████████████████████████████▏        | 897/1000 [17:42<04:27,  2.60s/it]\u001b[A\n",
      "Estimating norm scaling factor:  91%|█████████████████████████████████████████████████████████████████████████████        | 906/1000 [17:42<02:47,  1.78s/it]\u001b[A\n",
      "Estimating norm scaling factor:  92%|█████████████████████████████████████████████████████████████████████████████▊       | 916/1000 [17:42<01:39,  1.19s/it]\u001b[A\n",
      "Estimating norm scaling factor:  93%|██████████████████████████████████████████████████████████████████████████████▋      | 926/1000 [17:42<00:59,  1.24it/s]\u001b[A\n",
      "Estimating norm scaling factor:  94%|███████████████████████████████████████████████████████████████████████████████▌     | 936/1000 [17:42<00:35,  1.80it/s]\u001b[A\n",
      "Estimating norm scaling factor:  95%|████████████████████████████████████████████████████████████████████████████████▍    | 946/1000 [17:42<00:20,  2.59it/s]\u001b[A\n",
      "Estimating norm scaling factor:  96%|█████████████████████████████████████████████████████████████████████████████████▎   | 956/1000 [17:43<00:11,  3.69it/s]\u001b[A\n",
      "Estimating norm scaling factor:  96%|█████████████████████████████████████████████████████████████████████████████████▎   | 956/1000 [17:59<00:11,  3.69it/s]\u001b[A\n",
      "Estimating norm scaling factor:  96%|█████████████████████████████████████████████████████████████████████████████████▋   | 961/1000 [18:49<01:41,  2.61s/it]\u001b[A\n",
      "Estimating norm scaling factor:  97%|██████████████████████████████████████████████████████████████████████████████████▍  | 970/1000 [18:49<00:53,  1.79s/it]\u001b[A\n",
      "Estimating norm scaling factor:  98%|███████████████████████████████████████████████████████████████████████████████████▎ | 980/1000 [18:49<00:23,  1.19s/it]\u001b[A\n",
      "Estimating norm scaling factor:  99%|████████████████████████████████████████████████████████████████████████████████████▏| 990/1000 [18:49<00:08,  1.23it/s]\u001b[A\n",
      "Estimating norm scaling factor: 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:49<00:00,  1.13s/it]\u001b[A\n",
      "30000| l1_loss: 213.98141 | mse_loss: 221.19931: 100%|████████████████████████████████████████████████████| 122880000/122880000 [10:58:32<00:00, 3109.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.save_model('./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the trained SAE locally, you may want to upload it to an S3 bucket in your account for persistence. Be sure to follow [best practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html) for S3 bucket security if you save any data to your AWS account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
